---
layout: post
title: "人工智能原理与设计复习大纲"
date:   2025-06-15
tags: [学校课程复习,人工智能]
comments: true
toc: true
author: Ainski
---

# 人工智能原理与设计复习大纲
在本文档当中，将会记录复习中产生的所有文字资料


<!-- more -->
## 模块一：人工智能发展历史

### 证明算术公理的相容性
- 完备性：
- 一致性：一个命题不可能同时为真或者假
- 可判定性： 算法在有限步内可以判定命题的真伪

### 人工智能的研究领域

- 以符号主义为核心的 *逻辑推理*
- 以问题求解为核心的 *探寻搜索*
- 以数据驱动为核心的 *机器学习*
- 以行为主义为核心的 *强化学习*
- 以博弈对抗为核心的 *决策智能*

## 模块二：知识表达与推理——确定性知识表示与推理
### 命题逻辑
- `命题` 确定为真或者假的陈述句
- `复合命题` 包含其他命题作为其组成部分的命题，在这个命题当中包含了
    - and
    - or
    - not
    - conditional(逻辑蕴含)
    - bi-conditional (等价)
- 等价使用 `≡` 来表示
### 谓词逻辑
- $\forall \ \exists $ etc 这部分和离散数学重合很多

### 归结原理 
（重要，来自于离散数学2.3内容，当时课堂不讲）

- ***消蕴含和等价***: $ P \to Q = \neg P \vee Q $ 以及等价 $\leftrightarrow$
- ***否定内移***: 将最外面的符号移动到命题上
- ***变量标准化***：每个量词采用不同的变量 $(\forall x)P(x)=(\forall y) P(y)$
- ***消去存在量词***: 如果不在任何一个全称量词的辖域内$ (\exists x)P(x) \Rightarrow P(A) $ 否则，考虑使用skolem 函数表示依赖关系$\forall x\exists y P(x) \vee Q(y) \to \forall x P(x) \vee Q(F(x))$
- ***公式化为前束形*** 把所有的全称量词移动到公式的左边，并使每个量词的辖域包含这个量词后面的整个部分
- ***化为合取范式***
- ***略去全称量词***
- ***消去 $\wedge$*** 把母式用子句集表示
- ***字句变量标准化*** 要求每个字句中的变量符号不同

### 置换和合一
在子句集当中，
$P(x,y) \Leftrightarrow P(w,z)$可以进行如此置换，置换之后相同称之为 ***匹配*** ，因此可以合一。

### 归结原理
基本原理是所有的字句都是与的关系，因此要求所有的字句同时为真  
在归结之前，应当置换变量使他们包含互补的文字，才可以归结例如：
S={P(x) or Q(x) , not P(A) or R(y)}
S={P(x) or Q(x) , not P(x) or R(y)}
归结为 Q(x) or R(y)

***不能同时消去两个互补文字对***

归结原理的使用具体更可以参照离散数学当中的自然推理系统 

### 谓词解决实际问题
步骤如下：定义谓词，谓词公式说明（从题目当中找规则），生成子句集，归结原理归结


## 模块二：知识表达与推理——不确定性知识表达与推理

### 需要掌握的基本知识点

随机变量 基本命题 复合命题 概率分布 先验概率 联合概率分布 完全联合概率分布 条件概率 后验概率    
#### 乘法法则
$P(A \vee B) = P(A | B)P(B) =P(B|A)P(A)$   
#### 链式法则
$P(X_1,X_2...X_n)=\prod_{1}^{n} P(X_i|X_0X_1...X_{i-1})$

条件概率...详细参考上个学期概率论的内容 

#### 归一化方法 
将某一条件下的概率调整其和为1   
其实际运算过程与按照条件概率定义求值的过程基本一样

### 贝叶斯规则
$P(b|a)=\frac{P(a|b)P(b)}{P(a)}$    
这是一个知三求一的式子

### 贝叶斯网络

- 是一个有向无环图
- 表示变量之间的依赖关系
- 本质上，表示任何完全联合概率分布

某一事件发生的概率需要由它的父节点的概率求得    

#### 构建贝叶斯网络的方法

- 以任意次序排序
- 对于每个新加入图的节点，利用独立性检验找到最小父节点

## 搜索探寻与问题求解 —— 搜索树的构建与启发式搜索
这一部分的内容与算法后面的回溯算高度相似，可以参考使用

### 搜索的形式化描述
<状态，动作，状态转移，路径/代价，目标测试>     
< statement,action,transaction,cost,test >
- 状态 智能体当前的状态 s 
- 动作 从一个状态转移到另一个状态所采取的行为 a
- 状态转移 计算转移后的状态的函数 f(s,a) return s'
- 路径/代价 完成动作所需要的消耗 c(s,a,s')
- 目标测试 判定当前状态是否为目标状态 r(s)
### 搜索树
参考算法当中的解空间，两者有相同的定义。    
具体来讲，搜索树指的是从初始状态开始，以树状图的形式将所有的解罗列出来的方式。  
这种方式以偶某种弊端，要求遍历所有的解，因此时间复杂度在 **指数** 等级上    
而遍历的方法为广度为优先队列。

### 启发式搜索

#### 搜索方向
- 数据驱动 正向搜索，从初始状态出发
- 目的驱动 你想搜索，从想要达到的目的入手，看那些操作算子能产生该目的
- 双向搜索 从开始状态出发做正向搜索，也同时从目的状态出发做逆向搜索，直到两条路径在中间的某处交汇为止。

#### 无信息搜索

- 盲目搜索 简单可以解释为 c(s,a,s')=c 即无法判断某个状态消耗更多 通过暴力遍历图中的所有节点
- 包括宽度优先，深度优先，以及一致代价搜索（纯贪心，哪里的从c(s,a,s') 小就走哪里）

#### 评价函数
从原有的f(n) =g ( n)（一致代价搜索） 更新为f(n) = g(n) + h(n)（启发式搜索）
- g(n) = c(s,a,s') 实际代价，即从初始状态到当前状态的实际代价
- h(n) 启发式估计，即从当前状态到目标状态的估计代价

##### 贪婪最佳优先搜索
f(n) = h(n) 目的性极强的算法    

###### 性能
完备性？ x 会陷入死循环
最优性？ x 可能不一定是最优解
时间复杂度？ O(b^m) 一个好的启发函数可以有效降低复杂度
空间复杂度？ O(b^m) 存储搜索树

##### A*搜索
f(n)= h(n)+g(n) 

###### 可容性
如果对于每个节点n h(n)<= h*(n) 那么就是可采纳的 -> 这个版本的搜索树是最优的

###### 一致性
h(n) <= c(n,a,n')+h(n') 即f(n')>=f(n) f(n) 是递减的，因此会有一致性

## 搜索探寻与问题求解 —— 对抗搜索

### 博弈论
- player 参与博弈的决策主体
- strategy 参与者可以采取的行动方案
- strategy set 可以采取的策略集合
    - 混合策略 根据一定的概率分布
    - 纯策略 每次行动选择某个特定的策略
- outcome 采取行动之后形成的状态为 **局势**
- payoff 期望收益

### 题目类型
- 合作博弈
- 非合作博弈

- 静态博弈
- 动态博弈

- 完全信息
- 不完全信息

#### 本章主要讨论 双人轮流式的对弈，两人得到的信息是完备的，零和博弈（只能对一方有利）

### 变量定义
- S0 初始状态
- Player(s) 定义这个时候该谁行动
- Actions(s) 定义此状态下的合法移动集合
- Result(s,a) **转移模型** 定义行动的结果
- Terminal-Test(s) **终止测试**
- Utility(s,p) **效用函数** 在终止状态下的结果

### 博弈树
解空间

### minmax 算法

基本策略： 最大化自身利益，最小化对手的利益
#### 静态估计函数 f
有利于max的状态 f(p)>0 否则 f(p) <0

例如在井字棋当中，可以定义f(p) 为所有空格都放上max 的棋子之后，三字成线的总数减去 所有空格都放上min 的棋子之后，三字成线的总数

### 性能
- 完备性 v
- 最优性 v
- 时间复杂度 O(b^m)
- 空间复杂度 O(b^m)

### Alpha-Beta 算法

基本策略： 把博弈树生成和倒推估值结合起来进行，再根据一定的条件判定，有可能尽早修剪掉一些无用的分枝

在一边计算的过程当中，顺便估计父节点的取值范围，以此减少计算次数。

### Alpha-Beta剪枝算法完整实例演示（修正版）

让我们重新梳理整个Alpha-Beta剪枝过程，确保逻辑准确无误。以下是一个完整的三层博弈树示例，包含详细的搜索步骤和剪枝逻辑：


#### **博弈树结构与节点估值**
```
          A(根节点, Max)
        /      \
       B(Min)   C(Min)
     /   \    /   \
    D(Max) E(Max) F(Max) G(Max)
   / \   / \   / \   / \
  3  5  6  2  1  8  4  7  （叶子节点估值）
```


##### **Alpha-Beta剪枝详细过程**

###### **第一步：搜索左子树（节点B及其子节点）**
1. **节点D（Max）**  
   - 子节点值：3和5  
   - Max节点选择最大值：**5**  
   - 更新α=5（当前Max节点能确保的最小值）  

2. **节点E（Max）**  
   - 子节点值：6和2  
   - Max节点选择最大值：**6**  
   - 更新α=6（因为6 > 5）  

3. **节点B（Min）**  
   - 子节点D=5，E=6  
   - Min节点选择最小值：**5**  
   - 更新β=5（当前Min节点能接受的最大值）  


###### **第二步：搜索右子树（节点C及其子节点）**
1. **节点F（Max）**  
   - 子节点值：1和8  
   - Max节点选择最大值：**8**  
   - 由于8 > 当前α=5，更新α=8  

2. **节点G（Max）**  
   - 子节点值：4和7  
   - Max节点选择最大值：**7**  
   - 由于7 < 当前α=8，α保持不变（α=8）  

3. **节点C（Min）**  
   - 子节点F=8，G=7  
   - Min节点选择最小值：**7**  
   - 更新β=7（当前Min节点能接受的最大值）  


###### **第三步：根节点A的决策**
- 根节点A为Max节点，比较左右子节点：  
  - 左子树B返回值：**5**  
  - 右子树C返回值：**7**  
- Max节点选择最大值：**7**  


###### **剪枝发生的位置**
1. **节点B的剪枝**  
   - 当搜索到节点E时，E的最大值为6。由于B是Min节点且已找到D=5，B的最终值不可能超过5（β=5）。因此，**E的右子树（值2）被剪枝**，无需评估。


###### **可视化剪枝结果**
```
          A(Max, 7)
        /      \
       B(Min,5) C(Min,7)
     /   \    /   \
    D(5)  E(6) F(8) G(7)
   / \   /    / \  / \
  3  5  6    1  8 4   7 （虚线表示被剪枝的分支）
```


###### **关键结论**
1. **最优决策路径**：A → C → G  
2. **最终得分**：7  
3. **剪枝效率**：减少了1个节点的评估（原本需评估8个，实际评估7个）  
4. **核心逻辑验证**：  
   - Min节点始终选择子节点的最小值  
   - Max节点始终选择子节点的最大值  
   - 剪枝条件：当α ≥ β时，停止搜索当前分支  

### 不完美的实施决策
- 加权线性评价函数
    - 通过机器学习来完成对于评价函数权值的估计